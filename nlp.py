# -*- coding: utf-8 -*-
"""NLP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CFih7xQNevAJUcO_4g9CBUmb-e3fW-i3

# **Some basic NLP**
"""

#Displaying English, French and Spanish stopwords
import nltk as nltk
from nltk.tokenize import word_tokenize , sent_tokenize
nltk.download("stopwords")
from nltk.corpus import stopwords
stopwords.words('english')

stopwords.words('french')

stopwords.words('spanish')

nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')

#Lemmatizing 
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
lemmatizer=WordNetLemmatizer()

x="""believes belief this crossing cross """
y=word_tokenize(x)
nltk.pos_tag(y)
for w in y:
 print("Lemma for", w ,"is :", lemmatizer.lemmatize(w,pos="v"))

#Stem words from other languages using the SnowballStemmer
from nltk.stem import SnowballStemmer
#Langages supported By SnowballStemmer
print(" ".join(SnowballStemmer.languages))

#Stemming a french text
text="""Le traitement automatique du langage naturel est utilisé au quotidien pour des centaines d’usages différents"""
french_words=word_tokenize(text) #Word tokenizing
stemmer = SnowballStemmer("french") # Choosing a language
for i in french_words :
 print(stemmer.stem(i))

#Stemming a german text
text1="""L'elaborazione automatica del linguaggio naturale viene utilizzata quotidianamente per centinaia di usi divers"""
italian_words=word_tokenize(text1) #Word tokenizing
stemmer = SnowballStemmer("italian") # Choosing a language
for i in italian_words :
 print(stemmer.stem(i))

#Stemming an arabic text
text2="""O processamento automático de linguagem natural é usado diariamente para centenas de usos diferentes """
portuguese_words=word_tokenize(text2) #Word tokenizing
stemmer = SnowballStemmer("portuguese") # Choosing a language
for i in portuguese_words :
 print(stemmer.stem(i))

#Finding definitions and examples for AI
import nltk
from nltk.corpus import wordnet

syns = wordnet.synsets("AI")
print("Defination of the said word:")
print(syns[0].definition())
print("\nExamples of the word in use::")
print(syns[0].examples())

#Displaying the list of synonyms

synonyms = [] 
for syn in wordnet.synsets("AI"):
    for l in syn.lemmas():
        synonyms.append(l.name())
print(set(synonyms))

#Finding antonyms and synonyms
syn = list()
ant = list()
for synset in wordnet.synsets("Ugly"):
   for lemma in synset.lemmas():
      syn.append(lemma.name())    
      if lemma.antonyms():    
       ant.append(lemma.antonyms()[0].name())
print('Synonyms: ' + str(syn))
print('Antonyms: ' + str(ant))

#Tokenize sentences using sent_tokenize
sntc="""Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can"""
#Sentence Tokenization
sent_tokenize(sntc)

#Word tokenization
word_tokenize(sntc)

#Tokenize sentences using sent_tokenize.
phrase=""" Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of artificial intelligence or AI—concerned with giving computers the ability to understand text and spoken words in much the same way human beings can"""
sent_tokenize(phrase)

#Tokenize phrases in another language 
portuguese_sent="""O processamento automático de linguagem natural é usado diariamente para centenas de usos diferentes"""
sent_tokenize(portuguese_sent)

"""Observation :  

-"Sent_tokenize" splits a text into sentences.

-This intruction uses ponctuation (dots for example) to tokenize the sentences . 

-It doesn't take in consideration the langage .

"""

#Tokenizing sentence with abbreviation
sent1=""" Natural language processing (NLP) strives to build machines that understand and respond to text or voice data. In much the same way humans do"""
sent_tokenize(sent1)

"""Observation : 

Abbreviations are not taking in consideration when we tokenize sentences meaning , they don't disturb or have an impact on the tokenization.
"""

#Word Tokenization
word_tokenize(sent1)

"""Observation :

 

*   "Word_tokenise" splits in the sentences into words.


- Dots are considered as words.
- Brackets are considered as words.
-Commas are considered as words.
"""